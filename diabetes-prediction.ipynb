{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes prediction using synthesized health records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This notebook explores how to train a machine learning model to predict type 2 diabetes using synthesized patient health records.  The use of synthesized data allows us to learn about building a model without any concern about the privacy issues surrounding the use of real patient health records.\n",
    "#\n",
    "# To do\n",
    "#   None\n",
    "\n",
    "# Error\n",
    "#   None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This project is part of a series of code patterns pertaining to a fictional health care company called Example Health.  This company stores electronic health records in a database on a z/OS server.  Before running the notebook, the synthesized health records must be created and loaded into this database.  Another project, https://github.com/IBM/example-health-synthea, provides the steps for doing this.  The records are created using a tool called Synthea (https://github.com/synthetichealth/synthea), transformed and loaded into the database.\n",
    "\n",
    "import argparse, datetime, os, pdb, sys\n",
    "import math, statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "z_OS_server = False  # Database from z/OS server.\n",
    "NOTEBOOK = False\n",
    "\n",
    "EARLY_DEBUGGING = False\n",
    "DEBUGGING = True\n",
    "EARLY_TESTING = False\n",
    "TESTING = True\n",
    "\n",
    "TO_DO = False\n",
    "\n",
    "\n",
    "def getting_arg():\n",
    "    parser = argparse.ArgumentParser(description='Make data')\n",
    "    parser.add_argument('--in_file', dest='in_file',\n",
    "                        help='output file name')\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the information needed for a JDBC connection to your database below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The database must be set up by following the instructions in https://github.com/IBM/example-health-synthea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed when this code deployed in IBM Cloud\n",
    "credentials_1 = {\n",
    "  'host':'xxx.yyy.com',\n",
    "  'port':'nnnn',\n",
    "  'username':'user',\n",
    "  'password':'password',\n",
    "  'database':'location',\n",
    "  'schema':'SMHEALTH'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to load data from a database table into a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The partitionColumn, lowerBound, upperBound, and numPartitions options are used to load the data more quickly\n",
    "# using multiple JDBC connections.  The data is partitioned by patient id.  It is assumed that there are approximately\n",
    "# 5000 patients in the database.  If there are more or less patients, adjust the upperBound value appropriately.\n",
    "\n",
    "args = getting_arg().parse_args()\n",
    "\n",
    "if (any(vars(args).values()) == None):\n",
    "    print (\"Please, give me input arguments, thanks.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print (vars(args))\n",
    "\n",
    "no_ep = 100\n",
    "each_b_s = 100\n",
    "# Dataset will be divided and one group size will be batch size\n",
    "total_n_group = 10\n",
    "\n",
    "observations_and_condition_df = pd.read_pickle(args.in_file)\n",
    "# observations_and_condition_df = \\\n",
    "#     observations_and_condition_df.set_index('patientid')\n",
    "oac_original = observations_and_condition_df\n",
    "merged_observations_df = observations_and_condition_df[[\n",
    "    'patientid', 'dateofobservation', 'systolic', 'diastolic', 'hdl',\n",
    "    'ldl', 'bmi']]\n",
    "# +---------+-----------------+--------+---------+-----+------+-----+\n",
    "# |patientid|dateofobservation|systolic|diastolic|  hdl|   ldl|  bmi|\n",
    "# +---------+-----------------+--------+---------+-----+------+-----+\n",
    "# |        4|       2011-12-17|  105.10|    77.10|71.00| 86.50|57.70|\n",
    "\n",
    "# observations_and_condition_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the observations for diabetics to remove those taken before diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is driven by the way that the diabetes simulation works in Synthea.\n",
    "# The impact of the condition (diabetes) is not reflected\n",
    "# in the observations until the patient is diagnosed\n",
    "# with the condition in a wellness visit.\n",
    "# Prior to that the patient's observations\n",
    "# won't be any different from a non-diabetic patient.\n",
    "# Therefore we want only the observations at the time the patients were diabetic.\n",
    "\n",
    "# 1st trial\n",
    "# This generate only index\n",
    "# observations_and_condition_df = (\n",
    "#     observations_and_condition_df.filter(\n",
    "#         (observations_and_condition_df[\"diabetic\"] == 0) |\n",
    "#         ((observations_and_condition_df[\"dateofobservation\"] >=\n",
    "#         observations_and_condition_df[\"start\"])))\n",
    "# )\n",
    "\n",
    "# 2nd trial - original code\n",
    "# observations_and_condition_df = (\n",
    "#     observations_and_condition_df.filter(\n",
    "#     (col(\"diabetic\") == 0) |\n",
    "#     ((col(\"dateofobservation\") >= col(\"start\"))))\n",
    "# )\n",
    "\n",
    "# 3rd trial\n",
    "observations_and_condition_df = observations_and_condition_df[\n",
    "    (observations_and_condition_df[\"diabetic\"] == 0) |\n",
    "    (observations_and_condition_df[\"dateofobservation\"] >=\n",
    "    observations_and_condition_df[\"start\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the observations to a single observation per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient (the earliest available observation)\n",
    "# Sort by patient ID and date of observation\n",
    "w = pd.DataFrame(observations_and_condition_df.sort_values(\n",
    "\t\tby = ['patientid','dateofobservation'],\n",
    "\t\tascending = [True,True]))\n",
    "\n",
    "# Keep the earlist date of observation\n",
    "first_observation_df = pd.DataFrame(columns=w.keys())\n",
    "for i_dx, i in enumerate(w.iloc):\n",
    "    if (i_dx == 0):\n",
    "        first_observation_df = first_observation_df.append(i)\n",
    "    else:\n",
    "        if (i['patientid'] != i_pre['patientid']):\n",
    "            first_observation_df = first_observation_df.append(i)\n",
    "    i_pre = i\n",
    "\n",
    "if (EARLY_DEBUGGING):\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# At this point we have collected some observations which might be relevant to making a diabetes prediction.  The next step is to look for relationships between those observations and having diabetes.  There are many tools that help visualize data to look for relationships.  One of the easiest ones to use is called Pixiedust (https://github.com/pixiedust/pixiedust).\n",
    "#\n",
    "# Install the pixiedust visualization tool.\n",
    "# !pip install --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pixiedust to visualize whether observations correlate with diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The PixieDust interactive widget appears when you run this cell.\n",
    "# * Click the chart button and choose Scatter Plot.\n",
    "# * Click the chart options button.  Drag \"ldl\" into the Keys box and drag \"hdl\" into the Values box.\n",
    "# Set the # of Rows to Display to 5000.  Click OK to close the chart options.\n",
    "# * Select bokeh from the Renderer dropdown menu.\n",
    "# * Select diabetic from the Color dropdown menu.\n",
    "#\n",
    "# The scatter plot chart appears.\n",
    "#\n",
    "# Click Options and try replacing \"ldl\" and \"hdl\" with other attributes.\n",
    "\n",
    "if (NOTEBOOK):\n",
    "    import pixiedust\n",
    "\n",
    "    display(first_observation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The visualization of the data showed that the strongest predictors of diabetes are the cholesterol observations.  This is an artifact of the diabetes simulation used to create the synthesized data.  The simulation uses a distinct range of HDL readings for diabetic vs. non-diabetic patients.\n",
    "#\n",
    "# The simulation increases the chance of high blood pressure (hypertension) for diabetics but the non-diabetic patients also can have high blood pressure.  Therefore the correlation of high blood pressure to diabetes isn't very strong.\n",
    "#\n",
    "# The simulation does not change the weight of any diabetic patients so BMI has no correlation.\n",
    "#\n",
    "# Let's continue using HDL and systolic blood pressure as the features for the model.  In reality more features would be needed to build a usable model.\n",
    "#\n",
    "# Create a pipeline that assembles the feature columns and runs a logistic regression algorithm.  Then use the observation data to train the model.\n",
    "\n",
    "# vectorAssembler_features = VectorAssembler(inputCols=[\"hdl\", \"systolic\"],\n",
    "#     outputCol=\"features\")\n",
    "#\n",
    "# lr = LogisticRegression(featuresCol = 'features',\n",
    "#     labelCol = 'diabetic', maxIter=10)\n",
    "#\n",
    "# pipeline = Pipeline(stages=[vectorAssembler_features, lr])\n",
    "\n",
    "# DataFrame --> Numpy Array\n",
    "#  hdl  systolic  diabetic\n",
    "# 0   79       105         1\n",
    "# 1   79       105         0\n",
    "# 2   80       105         1\n",
    "# 3   79       104         1\n",
    "# 4   79       105         0\n",
    "#\n",
    "# array([[  1.,  79., 105.],\n",
    "#        [  0.,  79., 105.],\n",
    "#        [  1.,  80., 105.],\n",
    "#        [  1.,  79., 104.],\n",
    "#        [  0.,  79., 105.]])\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "first_observation_dict = \\\n",
    "    first_observation_df[[\"hdl\", \"systolic\", \"diabetic\"]].to_dict('records')\n",
    "x_y = vectorizer.fit_transform(first_observation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the observation data into two portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The larger portion (80% of the data) is used to train the model.\n",
    "# The smaller portion (20% of the data) is used to test the model.\n",
    "\n",
    "# Error spot\n",
    "# *** AttributeError: 'DataFrame' object has no attribute 'randomSplit'\n",
    "# split_data = first_observation_df.randomSplit([0.8, 0.2], 24)\n",
    "# train_data = split_data[0]\n",
    "# test_data = split_data[1]\n",
    "\n",
    "# ''' 80% and 20% '''\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_y[:, 1:], x_y[:, 0],\n",
    "    train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "unique = list(set(train_y))\n",
    "train_y_number = []\n",
    "for i in train_y:\n",
    "    train_y_number.append(unique.index(i))\n",
    "unique = list(set(test_y))\n",
    "test_y_number = []\n",
    "for i in test_y:\n",
    "    test_y_number.append(unique.index(i))\n",
    "\n",
    "# (1) Linear regression\n",
    "lr_1 = LinearRegression()\n",
    "lr_1.fit(train_x, train_y_number)  # x needs to be 2d for LinearRegression\n",
    "print(\"Accuracy = {:.2f}\".format(lr_1.score(test_x, test_y_number)))\n",
    "\n",
    "# (2) Logistic regression\n",
    "lr = LogisticRegressionCV()\n",
    "lr.fit(train_x, train_y)\n",
    "\n",
    "print(\"Accuracy = {:.2f}\".format(lr.score(test_x, test_y)))\n",
    "\n",
    "# (3) Neural Network\n",
    "# ''' Keras with one hidden layer and 16 units '''\n",
    "def one_hot_encode_object_array(arr):\n",
    "    uniques, ids = np.unique(arr, return_inverse=True)\n",
    "    return np_utils.to_categorical(ids, len(uniques))\n",
    "\n",
    "train_y_ohe = one_hot_encode_object_array(train_y)\n",
    "test_y_ohe = one_hot_encode_object_array(test_y)\n",
    "\n",
    "if (DEBUGGING):\n",
    "    pdb.set_trace()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(2,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# Error\n",
    "# sparse_categorical_crossentropy\n",
    "# tensorflow.python.framework.errors_impl.InvalidArgumentError:  logits and labels must have the same first dimension, got logits shape [393,2] and labels shape [786]\n",
    "# \"sparse_categorical_crossentropy\", 'categorical_crossentropy'\n",
    "\n",
    "each_b_s = int(train_y_ohe.shape[0] / total_n_group)\n",
    "\n",
    "if (TESTING):\n",
    "    print (\"Dataset size: {}, batch size: {}, total epoch: {}\".format(\n",
    "    train_y_ohe.shape[0], each_b_s, no_ep\n",
    "    ))\n",
    "model.fit(train_x, train_y_ohe, epochs=no_ep, batch_size=each_b_s, verbose=0);\n",
    "\n",
    "loss, accuracy = model.evaluate(test_x, test_y_ohe, verbose=0)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))\n",
    "\n",
    "# pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# One way to evaluate the model is to plot a precision/recall curve.\n",
    "#\n",
    "# Precision measures the percentage of the predicted true outcomes that are actually true.\n",
    "#\n",
    "# Recall measures the percentage of the actual true conditions that are predicted as true.\n",
    "#\n",
    "# Ideally we want both precision and recall to be 100%.\n",
    "# We want all of the diabetes predictions to actually have diabetes (precision = 1.0).\n",
    "# We want all of the actual diabetics to be predicted to be diabetic (recall = 1.0).\n",
    "#\n",
    "# The model computes the probability of a true condition and then compares that to a threshold\n",
    "# (by default 0.5) to make a final true of false determination.  The precision/recall curve plots\n",
    "# precision and recall at various threhold values.\n",
    "\n",
    "# Plot the model's precision/recall curve.\n",
    "\n",
    "if (NOTEBOOK):\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "if (TO_DO):\n",
    "    trainingSummary = model.stages[-1].summary\n",
    "\n",
    "    pr = trainingSummary.pr.toPandas()\n",
    "    plt.plot(pr['recall'],pr['precision'])\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.show()\n",
    "\n",
    "# Let's use the model to make predictions using the test data.  We'll leave the threshold for deciding between a true or false result at the default value of 0.5.\n",
    "# predictions = model.transform(test_data)\n",
    "\n",
    "x = test_x\n",
    "prediction = model.predict(\n",
    "    x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10,\n",
    "    workers=1, use_multiprocessing=True\n",
    ")\n",
    "\n",
    "if (TESTING):\n",
    "    pdb.set_trace()\n",
    "\n",
    "# Compute recall and precision for the test predictions to see how well the model does.\n",
    "tp, fp, fn = 0, 0, 0\n",
    "# 1 is diabetic in test label\n",
    "# Precition probability of non diabetic and probabilit of diabetic\n",
    "for i_dx, i in enumerate(prediction):\n",
    "    if (i[0] > i[1]):\n",
    "        cur_pred = 0\n",
    "    else:\n",
    "        cur_pred = 1\n",
    "\n",
    "    if (cur_pred == 1 and test_y[i_dx]) == 1:\n",
    "        tp += 1\n",
    "    elif (cur_pred == 1 and test_y[i_dx]) == 0:\n",
    "        fp += 1\n",
    "    elif (cur_pred == 0 and test_y[i_dx]) == 1:\n",
    "        fn += 1\n",
    "\n",
    "print(\"True positives  = %s\" % tp)\n",
    "print(\"False positives = %s\" % fp)\n",
    "print(\"False negatives = %s\" % fn)\n",
    "\n",
    "print(\"Recall = %s\" % (tp / (tp + fn)))\n",
    "print(\"Precision = %s\" % (tp / (tp + fp)))\n",
    "\n",
    "\n",
    "if (DEPLOY):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish and deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "    # In this section you will learn how to store the model in the Watson Machine Learning repository by using the repository client.\n",
    "    #\n",
    "    # First install the client library.\n",
    "\n",
    "\n",
    "    get_ipython().system('rm -rf $PIP_BUILD/watson-machine-learning-client')\n",
    "    get_ipython().system('pip install watson-machine-learning-client --upgrade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your Watson Machine Learning service instance credentials here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They can be found in the Service Credentials tab of\n",
    "    # the Watson Machine Learning service instance that you created on IBM Cloud.\n",
    "\n",
    "    wml_credentials={\n",
    "      \"url\": \"https://xxx.ibm.com\",\n",
    "      \"username\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n",
    "      \"password\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n",
    "      \"instance_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish the model to the repository using the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "    client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "\n",
    "    model_props = {\n",
    "        client.repository.ModelMetaNames.NAME: \"diabetes-prediction-1\",\n",
    "    }\n",
    "\n",
    "    stored_model_details = client.repository.store_model(model, meta_props=model_props, training_data=train_data, pipeline=pipeline)\n",
    "\n",
    "    model_uid            = client.repository.get_model_uid( stored_model_details )\n",
    "    print( \"model_uid: \", model_uid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model as a web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_details = client.deployments.create(model_uid, 'diabetes-prediction-1 deployment')\n",
    "\n",
    "    scoring_endpoint = client.deployments.get_scoring_url(deployment_details)\n",
    "    print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the web service to make a prediction from some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_payload = {\n",
    "        \"fields\": [\"hdl\", \"systolic\"],\n",
    "        \"values\": [[45.0, 156.6]]\n",
    "    }\n",
    "\n",
    "    score = client.deployments.score(scoring_endpoint, scoring_payload)\n",
    "\n",
    "    print(str(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:grad]",
   "language": "python",
   "name": "conda-env-grad-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
